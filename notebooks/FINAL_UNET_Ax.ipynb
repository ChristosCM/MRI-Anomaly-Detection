{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FINAL_UNET_Ax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "e7sMXqbtuEAD",
        "outputId": "0a6a2f31-28de-491f-caed-7d45b5e79ad9"
      },
      "source": [
        "!pip install -q --upgrade wandb==0.10.8\n",
        "import wandb\n",
        "wandb.init(project=\"OriginalUnet\",name=\"Axial-2nd\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7MB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 21.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 24.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.29 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.8<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">Axial-2nd</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ccm/OriginalUnet\" target=\"_blank\">https://wandb.ai/ccm/OriginalUnet</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/ccm/OriginalUnet/runs/2fqrit0l\" target=\"_blank\">https://wandb.ai/ccm/OriginalUnet/runs/2fqrit0l</a><br/>\n",
              "                Run data is saved locally in <code>wandb/run-20210505_134631-2fqrit0l</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fd0e95ce890>"
            ],
            "text/html": [
              "<h1>Run(2fqrit0l)</h1><p></p><iframe src=\"https://wandb.ai/ccm/OriginalUnet/runs/2fqrit0l\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdhdVuk2uKHc",
        "outputId": "e853d698-7447-4acf-847e-a2c1148349c1"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm_notebook\n",
        "import random\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "import h5py\n",
        "from IPython.display import clear_output\n",
        "from math import floor\n",
        "from torchsummary import summary\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjOBVSfSuLKT"
      },
      "source": [
        "img_width=256\n",
        "img_height=192\n",
        "wb_size=16\n",
        "white_box = torch.ones(wb_size,wb_size)\n",
        "# center = img_width//3\n",
        "center=0\n",
        "def randomMask(image): #the random mask is still bounded \n",
        "    # white_box = np.array([255]*size*size).reshape(size,size)\n",
        "    x= np.random.randint(center,img_width-wb_size-center, size=1)[0] #42 in so that the image is placed on the middle third \n",
        "    y= np.random.randint(center,img_height-wb_size-center, size=1)[0] \n",
        "    image[0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    # mask = torch.zeros((1,img_width,img_width))\n",
        "    # mask[0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    return image#,mask\n",
        "def centerMask(image):\n",
        "    x,y = ((128//2)-8),((128//2)-8)\n",
        "    image[0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    return image\n",
        "def customBatchMask(images,positions):\n",
        "    for i in range(images.shape[0]):\n",
        "        x = floor(positions[i][0][0][0])\n",
        "        y = floor(positions[i][1][0][0])\n",
        "        images[i,0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    return images\n",
        "def customMask(img,position):\n",
        "    \n",
        "    x = position[0]\n",
        "    y = position[1]\n",
        "    if x+wb_size>=128:\n",
        "        x=127-wb_size\n",
        "    if y+wb_size>=128:\n",
        "        y=127-wb_size\n",
        "    img[0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    mask = torch.zeros((1,img_width,img_width))\n",
        "    mask[0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    return img, mask\n",
        "def recon_mask(image):\n",
        "    x, y = np.random.randint(42,img_width-wb_size-42, size=2) #42 in so that the image is placed on the middle third \n",
        "    image[0,x:x+wb_size, y:y+wb_size] = white_box\n",
        "    return image, (x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWG1AoRFuMhJ"
      },
      "source": [
        "class dataset_h5(torch.utils.data.Dataset):\n",
        "    def __init__(self, in_file, transform=None):\n",
        "        super(dataset_h5, self).__init__()\n",
        " \n",
        "        self.file = h5py.File(in_file, 'r')\n",
        "        self.transform = transform\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        x = self.file['X_train'][index]\n",
        "        p = self.file[\"X_train\"][index]\n",
        "        # Preprocessing each image\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)   \n",
        "            p = self.transform(p)     \n",
        "            p = randomMask(p)\n",
        "        return x,p#,mask #x is original and p is inpainted\n",
        " \n",
        "    def __len__(self):\n",
        "        return self.file['X_train'].shape[0]\n",
        "\n",
        "img_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n",
        "train_set = dataset_h5(\"/content/drive/MyDrive/datasets/2d_data/NFBS_2d_Axial_h5/data_axial.h5\",transform=img_transform)\n",
        "Path(\"outputs/\").mkdir(parents=True, exist_ok=True) #generate path to save produced images\n",
        "\n",
        "# dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,drop_last=False, shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD53yNGWuVJk"
      },
      "source": [
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "num_workers = 1\n",
        "batch_size = 8\n",
        "dataloader = DataLoader(train_set, batch_size=batch_size,drop_last=False, shuffle=True, num_workers=num_workers)\n",
        "n_channels = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vbIs95kuW4_",
        "outputId": "f8552f5a-acf7-4f11-d26f-36867826fa5e"
      },
      "source": [
        "# define unet model\n",
        "class UNet(nn.Module):\n",
        "  \n",
        "    def __init__(self, in_class=1, out_class=1,f=64):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.dconv_down1 = self.conv_block(in_class, f)\n",
        "        self.dconv_down2 = self.conv_block(f, f*2)\n",
        "        self.dconv_down3 = self.conv_block(f*2, f*4)\n",
        "        self.dconv_down4 = self.conv_block(f*4, f*8)\n",
        "        self.dconv_down5 = self.conv_block(512, 1024)\n",
        "\n",
        "        self.dconv_up4 = self.conv_block(512 + 1024, 512)\n",
        "        self.dconv_up3 = self.conv_block(f*4 + f*8, f*4)\n",
        "        self.dconv_up2 = self.conv_block(f*2 + f*4, f*2)\n",
        "        self.dconv_up1 = self.conv_block(f*2 + f, f)\n",
        "        \n",
        "        self.conv_last = nn.Conv2d(f, out_class, 1)\n",
        "        \n",
        "    def conv_block(self, in_channels, out_channels,mid=None):\n",
        "        if mid==None:\n",
        "            mid=out_channels\n",
        "        else:\n",
        "            mid=in_channels//2\n",
        "        return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid, 3, padding=1),\n",
        "        nn.BatchNorm2d(mid),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(mid, out_channels, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )   \n",
        "        \n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x) # skip-connection 1\n",
        "        x = F.max_pool2d(conv1, 2)\n",
        "        conv2 = self.dconv_down2(x) # skip-connection 2\n",
        "        x = F.max_pool2d(conv2, 2)\n",
        "        conv3 = self.dconv_down3(x) # skip-connection 3\n",
        "        x = F.max_pool2d(conv3, 2)\n",
        "\n",
        "        conv4 = self.dconv_down4(x) # skip-connection 4\n",
        "        x = F.max_pool2d(conv4, 2)\n",
        "\n",
        "        x = self.dconv_down5(x) # skip-connection 4\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        \n",
        "        x = torch.cat([x, conv4], dim=1)\n",
        "        x = self.dconv_up4(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.dconv_up3(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.dconv_up2(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        x = self.dconv_up1(x)\n",
        "        out = self.conv_last(x)\n",
        "        \n",
        "        return out\n",
        "\n",
        "N = UNet().to(device)\n",
        "print(f'> Number of network parameters {len(torch.nn.utils.parameters_to_vector(N.parameters()))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Number of network parameters 31389569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr4bmOD_uYxN",
        "outputId": "e3e6f8f1-ab62-46cf-fc0b-789107b0e669"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,f=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminate = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(1, f,3,2,padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(f, f*2,3,2,padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # nn.BatchNorm2d(f*2),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(f*2, f*4,3,2,padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # nn.BatchNorm2d(f*4),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(f*4, f*8,3,2,padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # nn.BatchNorm2d(f*8),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(f*8, 1,3,2,padding=1)),\n",
        "            nn.Sigmoid()\n",
        "                     )\n",
        "D = Discriminator().to(device)\n",
        "lossBCE = nn.BCELoss()\n",
        "summary(D.discriminate,(1,256,192))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 64, 128, 96]             640\n",
            "         LeakyReLU-2          [-1, 64, 128, 96]               0\n",
            "            Conv2d-3          [-1, 128, 64, 48]          73,856\n",
            "         MaxPool2d-4          [-1, 128, 32, 24]               0\n",
            "         LeakyReLU-5          [-1, 128, 32, 24]               0\n",
            "            Conv2d-6          [-1, 256, 16, 12]         295,168\n",
            "         MaxPool2d-7            [-1, 256, 8, 6]               0\n",
            "         LeakyReLU-8            [-1, 256, 8, 6]               0\n",
            "            Conv2d-9            [-1, 512, 4, 3]       1,180,160\n",
            "        MaxPool2d-10            [-1, 512, 2, 1]               0\n",
            "        LeakyReLU-11            [-1, 512, 2, 1]               0\n",
            "           Conv2d-12              [-1, 1, 1, 1]           4,609\n",
            "          Sigmoid-13              [-1, 1, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 1,554,433\n",
            "Trainable params: 1,554,433\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 17.13\n",
            "Params size (MB): 5.93\n",
            "Estimated Total Size (MB): 23.24\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecWlNLi-ubW9"
      },
      "source": [
        "train_iterator = cycle(dataloader)\n",
        "# wb_optimiser = torch.optim.Adam(wb_gen.parameters(), lr=0.001)\n",
        "optimiser_D = torch.optim.Adam(D.parameters(), lr=0.001)#, betas=(0.5, 0.999))\n",
        "optimiser_G = torch.optim.Adam(N.parameters(), lr=0.0001)\n",
        "gen_lambda = 50\n",
        "# G_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser_G, mode='min', factor=0.1, threshold=0.0001, patience=5)\n",
        "# D_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser_D, mode='min', factor=0.1, threshold=0.0001, patience=5)\n",
        "step=0\n",
        "lossL1 = nn.L1Loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQLZUROOucKc",
        "outputId": "1e23f95c-45f6-455e-f30d-fd51ec85b5e6"
      },
      "source": [
        "# training loop, feel free to also train on the test dataset if you like\n",
        "while (step<300):\n",
        "    \n",
        "    # arrays for metrics\n",
        "    train_loss_arr = np.zeros(0)\n",
        "    dis_loss_arr = np.zeros(0)\n",
        "    # iterate over some of the train dateset\n",
        "    for i in range(50):\n",
        "\n",
        "        \n",
        "        #TRAIN DISCRIMINATOR\n",
        "        # for _ in range(3):\n",
        "        x,p = next(train_iterator) #the original and the inpainted one\n",
        "        x = x.to(device)\n",
        "        p = p.to(device)\n",
        "        g = N(p).to(device)\n",
        "        optimiser_D.zero_grad()\n",
        "\n",
        "        l_r = lossBCE(D.discriminate(x).mean(), torch.ones(1)[0].to(device)) # real -> 1\n",
        "        l_f = lossBCE(D.discriminate(g).mean(), torch.zeros(1)[0].to(device)) #  fake -> 0\n",
        "        loss_d = (l_r + l_f)/2.0\n",
        "        loss_d.backward()\n",
        "        optimiser_D.step()\n",
        "        \n",
        "\n",
        "        #TRAIN GENERATOR \n",
        "        x,p = next(train_iterator) #the original and the inpainted one\n",
        "        x = x.to(device)\n",
        "        p = p.to(device)\n",
        "        g = N(p).to(device)\n",
        "        optimiser_G.zero_grad()\n",
        "        gen_loss = lossL1(g,x)*gen_lambda + lossBCE(D.discriminate(g).mean(), torch.ones(1)[0].to(device))\n",
        "        gen_loss.backward()\n",
        "        optimiser_G.step()\n",
        "\n",
        "        #ADD THE LOSSES TO ARRAY\n",
        "        dis_loss_arr = np.append(dis_loss_arr, loss_d.item())\n",
        "        train_loss_arr = np.append(train_loss_arr, gen_loss.item())\n",
        "\n",
        "\n",
        "    wandb.log({ 'UNET Loss':train_loss_arr.mean(),'discriminator loss': dis_loss_arr.mean()})\n",
        "\n",
        "    imglist = []\n",
        "    for j in range(5):\n",
        "        imglist.extend([x[j],p[j],g[j]])\n",
        "    plt.figure(figsize = (30,30))\n",
        "    im = torchvision.utils.make_grid(imglist,nrow=3).cpu().data.permute(0,2,1).contiguous().permute(2,1,0)\n",
        "    im = plt.imshow(torchvision.utils.make_grid(imglist,nrow=3).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "    # plt.show()\n",
        "    # plt.pause(0.0001)\n",
        "    name =\"outputs/epoch\"+\"_\"+str(step)+\".png\"\n",
        "    plt.savefig(name)\n",
        "    # f, ax = plt.subplots(1,figsize=(30,30))\n",
        "    # im = torchvision.utils.make_grid(imglist,nrow=3).cpu().data.permute(0,2,1).contiguous().permute(2,1,0)\n",
        "    # ax.imshow(im,cmap=plt.cm.binary)\n",
        "    wandb.save(name)\n",
        "\n",
        "    step += 1\n",
        "    # G_scheduler.step(0)\n",
        "\n",
        "    # D_scheduler.step(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH-_i99M6nUX"
      },
      "source": [
        "#save model to reuse for reconstruction\n",
        "def save_models_wandb():\n",
        "    Path(\"model_weights/\").mkdir(parents=True, exist_ok=True) #generate path to save produced images\n",
        "\n",
        "    torch.save(N,\"model_weights/unet-2.h5\")\n",
        "    torch.save(D,\"model_weights/discriminator-2.h5\")\n",
        "    wandb.save(\"model_weights/unet-2.h5\")\n",
        "    wandb.save(\"model_weights/discriminator-2.h5\")\n",
        "# save_models()\n",
        "# def save_models_neptune():\n",
        "#     torch.save(N,\"unet.h5\")\n",
        "#     torch.save(D,\"discriminator.h5\")\n",
        "#     neptune.log_artifact(\"unet.h5\")\n",
        "#     neptune.log_artifact(\"discriminator.h5\")\n",
        "save_models_wandb()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}